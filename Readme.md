# Run n8n & Ollama via Kubernetes on your macOS

# 0. Prerequisites
mac book air m3/m4 (above 24GB memory)<br>

# 1. Goal
We will run Local LLM on Kubernetes and create an Workflow for AI Agent using n8n.

<img src="https://github.com/developer-onizuka/n8n-ollama/blob/main/n8n-agent.png" width="960">

# 2. Procedure
### 2-1. Install Hypervisor
>https://www.oracle.com/jp/virtualization/technologies/vm/downloads/virtualbox-downloads.html

### 2-2. Install Vagrant
>https://developer.hashicorp.com/vagrant/install

### 2-3. Install git & git clone
>https://git-scm.com/downloads
```
git clone https://github.com/developer-onizuka/n8n-ollama
cd n8n-ollama
```
### 2-4. Roll out Virtual Machine
### 2-4-1. Master node / Worker node
```
cd kubernetes
vagrant up
cd ..
```
### 2-4-2. NFS Server
```
cd nfs
vagrant up
cd ..
```
### 2-5. Login Master node & git clone
```
cd kubernetes
vagrant ssh master
git clone https://github.com/developer-onizuka/n8n-ollama
cd n8n-ollama
```
### 2-6. Confirm Kubernetes Cluster
```
kubectl get nodes
```
```
$ kubectl get nodes
NAME      STATUS   ROLES           AGE   VERSION
master    Ready    control-plane   39m   v1.33.5
worker1   Ready    node            38m   v1.33.5
```
### 2-7. Setup LoadBalancer
Specify the range of IP addresses to be assigned to the load balancer.
```
kubectl apply -f metallb-ipaddress.yaml
```
### 2-8. Install CSI driver for NFS
```
./install-csi-driver.sh
```
### 2-9. Roll out StorageClass
```
kubectl apply -f storageclass-vm-nfs.yaml
kubectl apply -f storageclass-vm-nfs-n8n.yaml
```
```
$ kubectl get sc
NAME                       PROVISIONER      RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
nfs-vm-csi (default)       nfs.csi.k8s.io   Delete          Immediate           false                  8s
nfs-vm-csi-n8n (default)   nfs.csi.k8s.io   Delete          Immediate           false                  6s
```
### 2-10. Roll out PV
```
kubectl apply -f pvc-nfs-ollama.yaml
kubectl apply -f pvc-nfs-n8n.yaml
```
```
$ kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                    STORAGECLASS     VOLUMEATTRIBUTESCLASS   REASON   AGE
pvc-6614e902-7abc-4865-acc9-f2420f338daa   5Gi        RWX            Delete           Bound    default/pvc-nfs-n8n      nfs-vm-csi-n8n   <unset>                          66s
pvc-8b0667ba-1848-40e5-89f5-0ecaaa0c901d   20Gi       RWX            Delete           Bound    default/pvc-nfs-ollama   nfs-vm-csi       <unset>                          4s
```
### 2-11. Roll out Ollama & n8n
```
kubectl apply -f ollama.yaml
kubectl apply -f n8n.yaml
```
```
$ kubectl get pods -o wide
NAME                      READY   STATUS    RESTARTS   AGE     IP              NODE      NOMINATED NODE   READINESS GATES
n8n-5967965bc6-zwg7n      1/1     Running   0          3m29s   10.10.235.131   worker1   <none>           <none>
ollama-6c988c64c6-gzsck   1/1     Running   0          4m38s   10.10.235.130   worker1   <none>           <none>
```

If you find the error below, you might need to do "**sudo chown -R 1000:1000 /home/vagrant/exports-n8n**" on your NFS server before applying n8n.yaml.
```
$ kubectl logs n8n-5967965bc6-zwg7nÂ 
No encryption key found - Auto-generating and saving to: /home/node/.n8n/config
Error: EACCES: permission denied, mkdir '/home/node/.n8n'
```

### 2-12. Confirm Services
```
kubectl get services
```
```
$ kubectl get services -o wide
NAME         TYPE           CLUSTER-IP      EXTERNAL-IP    PORT(S)          AGE     SELECTOR
kubernetes   ClusterIP      10.96.0.1       <none>         443/TCP          48m     <none>
svc-n8n      LoadBalancer   10.96.251.15    192.168.33.2   5678:31211/TCP   4m13s   app=n8n
svc-ollama   ClusterIP      10.104.132.75   <none>         11434/TCP        5m22s   app=ollama
```
# 3. Download the model in Ollama as n8n backend
```
kubectl exec -it <your-ollama-pod-name> -- ollama pull llama3.2:3b
```
```
$ kubectl exec -it pods/ollama-6c988c64c6-7fxfp -- ollama pull llama3.2:3b
pulling manifest 
pulling dde5aa3fc5ff: 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 2.0 GB                         
pulling 966de95ca8a6: 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 1.4 KB                         
pulling fcc5a6bec9da: 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 7.7 KB                         
pulling a70ff7e570d9: 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 6.0 KB                         
pulling 56bb8bd477a5: 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   96 B                         
pulling 34bb5ab01051: 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  561 B                         
verifying sha256 digest 
writing manifest 
success
```
# 4. Create Workflow with n8n
### 4-1. Click Create Workflow
<img src="https://github.com/developer-onizuka/n8n-ollama/blob/main/n8n-create-workflow.png" width="720">

### 4-2. Add first step
Drow like below and select the Ollama Chat Model.:<br><br>
<img src="https://github.com/developer-onizuka/n8n-ollama/blob/main/n8n-myflow.png" width="720">

Set the following:
Model: "gemma3:1b"<br><br>
<img src="https://github.com/developer-onizuka/n8n-ollama/blob/main/n8n-model.png" width="720">

Credentials to connect with: See image below<br><br>
<img src="https://github.com/developer-onizuka/n8n-ollama/blob/main/n8n-ollama.png" width="720">

# 5. Testing your workflow
Type message and send it. So, you can find the picture as following and it means your workfrow works:<br><br>
<img src="https://github.com/developer-onizuka/n8n-ollama/blob/main/n8n-prompt.png" width="720">

# 6. AI Agent node with tools

I'll begin by explaining how LLMs and tools work together.Many LLMs, especially modern models like GPT-4 and Gemini, support a feature called Function Calling or Tool Calling. Here's how this process works:<br>
- First, in an n8n AI agent node, you connect tool nodes such as SerpApi. This action tells the LLM about the tool's functions (e.g., Google Search) and its required arguments (e.g., a query).<br>
- The n8n node then sends the user's input (e.g., "What's the current temperature?") along with the defined tool information to the LLM. The LLM analyzes the prompt and determines that this is a question requiring an external search.<br>
- Next, instead of generating a direct answer, the LLM produces special JSON data or text. This data takes the form of an instruction to "call the Google Search tool with 'current temperature' as the query."<br>
- Afterward, the n8n AI agent node interprets the JSON from the LLM. It recognizes that the LLM has decided a tool should be called. The n8n node then uses the specified tool (SerpApi) and arguments (query: 'current temperature') to actually perform the Google search.<br>
- Once the Google search results are retrieved, the n8n node sends those results back to the LLM.<br>
Finally, the LLM uses the provided search results to generate a final answer that is natural and easy for the user to understand (e.g., "The current temperature in Tokyo is...").<br>

When LLMs decide they need to use a tool, it might seem like a vague process at first glance. However, it's actually a logical reasoning process based on the model's training data and the design of its Tool Calling feature. This is because LLMs are not just predicting words probabilistically; they can also break down complex tasks and determine which tool is best suited to solve them.<br>
To be more specific, the n8n node sends the LLM not only the user's input but also a description of the available tools (e.g., Google Search, Image Generator) and an explanation of what each tool is for. This information acts as a clear set of rules that guides the LLM's reasoning. For example, the Google Search tool might have a description that says, "Use this to search for up-to-date information, real-time data, and website content."<br>
In addition, modern LLMs like GPT and Gemini are trained on massive datasets that include patterns of answering questions by referencing external sources. This teaches the model that questions like "What's the weather today?" are tasks that require an external tool, not just internal knowledge. At the same time, the LLM analyzes the user's intent from the prompt. For "What's the weather today?", it analyzes the intent as a request for the latest real-time information. For a question like "What is the capital of Japan?", it can determine that the answer is based on known knowledge. This process is similar to how a person, when asked "What's the weather today?", wouldn't just rely on their memory but would instead check a weather app on their phone. The LLM understands the nature of the task and selects the appropriate tool, just like a person would choose their phone.<br>
While the LLM's decision is probabilistic, behind that probability is strong evidence that calling a tool is the most rational and efficient solution. Therefore, it's not just a vague guess; it's a logical judgment based on a clear intent.<br><br>

æœ€åˆã«ã€LLMã¨ãƒ„ãƒ¼ãƒ«ã®é€£æºã®ä»•çµ„ã¿ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚å¤šãã®LLMã€ç‰¹ã«GPT-4ã‚„Geminiã®ã‚ˆã†ãªæœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ã€Œé–¢æ•°å‘¼ã³å‡ºã—ï¼ˆFunction Callingï¼‰ã€ ã¾ãŸã¯ ã€Œãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ï¼ˆTool Callingï¼‰ã€ ã¨å‘¼ã°ã‚Œã‚‹æ©Ÿèƒ½ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚ã“ã®æ©Ÿèƒ½ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªä»•çµ„ã¿ã§å‹•ä½œã—ã¾ã™ã€‚
- n8nã®AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒãƒ¼ãƒ‰ã§ã¯ã€SerpApiãªã©ã®ãƒ„ãƒ¼ãƒ«ãƒãƒ¼ãƒ‰ã‚’æ¥ç¶šã™ã‚‹ã“ã¨ã§ã€ãã®ãƒ„ãƒ¼ãƒ«ã®æ©Ÿèƒ½ï¼ˆä¾‹: Google Searchï¼‰ã¨ã€å¿…è¦ãªå¼•æ•°ï¼ˆä¾‹: queryï¼‰ã‚’LLMã«ä¼ãˆã‚‹ã“ã¨ã‹ã‚‰å§‹ã‚ã¾ã™ã€‚<br>
- ã¾ãšã€n8nãƒãƒ¼ãƒ‰ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ï¼ˆä¾‹: ä»Šã®æ°—æ¸©ã‚’çŸ¥ã‚ŠãŸã„ï¼‰ã¨ã€ä¸Šè¨˜ã§å®šç¾©ã—ãŸãƒ„ãƒ¼ãƒ«æƒ…å ±ã‚’LLMã«é€ä¿¡ã—ã¾ã™ã€‚LLMã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’åˆ†æã—ã€ã€Œã“ã‚Œã¯å¤–éƒ¨æ¤œç´¢ãŒå¿…è¦ãªè³ªå•ã ã€ã¨åˆ¤æ–­ã—ã¾ã™ã€‚<br>
- æ¬¡ã«ã€LLMã¯ç›´æ¥çš„ãªå›ç­”ã‚’ç”Ÿæˆã™ã‚‹ã®ã§ã¯ãªãã€ã€ŒGoogle Searchã¨ã„ã†ãƒ„ãƒ¼ãƒ«ã‚’queryã«ä»Šã®æ°—æ¸©ã¨ã„ã†å¼•æ•°ã§å‘¼ã³å‡ºã™ã€ ã¨ã„ã†å½¢å¼ã®ç‰¹æ®ŠãªJSONãƒ‡ãƒ¼ã‚¿ã‚„ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚<br>
- ãã®å¾Œã€n8nã®AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒãƒ¼ãƒ‰ã¯LLMã‹ã‚‰è¿”ã•ã‚ŒãŸã“ã®JSONã‚’è§£é‡ˆã—ã€LLMãŒã€Œãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã™ã¹ãã€ã¨åˆ¤æ–­ã—ãŸã¨èªè­˜ã—ã€AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒãƒ¼ãƒ‰ã¯æŒ‡å®šã•ã‚ŒãŸãƒ„ãƒ¼ãƒ«ï¼ˆSerpApiï¼‰ã¨å¼•æ•°ï¼ˆquery: 'ä»Šã®æ°—æ¸©'ï¼‰ã‚’ä½¿ã£ã¦ã€å®Ÿéš›ã«Googleæ¤œç´¢ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚<br>
- Googleæ¤œç´¢ã®çµæœãŒå–å¾—ã•ã‚Œã‚‹ã¨ã€n8nãƒãƒ¼ãƒ‰ã¯ãã®çµæœã‚’å†ã³LLMã«é€ã‚Šã¾ã™ã€‚<br>
- æœ€çµ‚çš„ã«ã€LLMã¯æä¾›ã•ã‚ŒãŸæ¤œç´¢çµæœã‚’åŸºã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¨ã£ã¦è‡ªç„¶ã§åˆ†ã‹ã‚Šã‚„ã™ã„æœ€çµ‚çš„ãªå›ç­”ï¼ˆä¾‹: ç¾åœ¨ã®æ±äº¬ã®æ°—æ¸©ã¯ã€‡ã€‡åº¦ã§ã™ã€‚ï¼‰ã‚’ç”Ÿæˆã—ã¾ã™ã€‚<br>

ãªãŠã€LLMãŒã€Œãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã™ã¹ãã€ã¨åˆ¤æ–­ã™ã‚‹éç¨‹ã¯ã€ä¸€è¦‹ã™ã‚‹ã¨æ›–æ˜§ãªãƒ—ãƒ­ã‚»ã‚¹ã®ã‚ˆã†ã«æ„Ÿã˜ã¾ã™ãŒã€ãã†ã§ã¯ãªãã€ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ã€Tool Callingæ©Ÿèƒ½ã®è¨­è¨ˆã«åŸºã¥ã„ãŸè«–ç†çš„ãªæ¨è«–ãƒ—ãƒ­ã‚»ã‚¹ã§ã™ã€‚ã“ã‚Œã¯ã€LLMãŒå˜èªã®ç¢ºç‡è«–çš„äºˆæ¸¬ã‚’è¡Œã†ã ã‘ã§ãªãã€è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã‚’åˆ†è§£ã—ã€ã©ã®ãƒ„ãƒ¼ãƒ«ãŒãã®ã‚¿ã‚¹ã‚¯ã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«æœ€ã‚‚é©ã—ã¦ã„ã‚‹ã‹ã‚’åˆ¤æ–­ã™ã‚‹èƒ½åŠ›ã‚’æŒã£ã¦ã„ã‚‹ãŸã‚ã§ã™ã€‚<br>
ã‚‚ã†å°‘ã—è©³ã—ãèª¬æ˜ã™ã‚‹ã¨ã€n8nã®ãƒãƒ¼ãƒ‰ã¯ã€LLMã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã ã‘ã§ãªãã€åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ã®æ©Ÿèƒ½ï¼ˆä¾‹: Google Searchã€image_generatorãªã©ï¼‰ã¨ã€ãã‚Œã‚‰ãŒã©ã®ã‚ˆã†ãªç›®çš„ã§ä½¿ã‚ã‚Œã‚‹ã¹ãã‹ã®èª¬æ˜ã‚’åŒæ™‚ã«æ¸¡ã—ã¾ã™ã€‚ã“ã®æƒ…å ±ãŒã€LLMã®æ¨è«–ã‚’ã‚¬ã‚¤ãƒ‰ã™ã‚‹ã€Œæ˜ç¢ºãªãƒ«ãƒ¼ãƒ«ã‚»ãƒƒãƒˆã€ã¨ãªã‚Šã¾ã™ã€‚ä¾‹ãˆã°ã€Google Searchãƒ„ãƒ¼ãƒ«ã«ã¯ã€Œæœ€æ–°ã®æƒ…å ±ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿ã€ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ¤œç´¢ã™ã‚‹ãŸã‚ã«ä½¿ã£ã¦ãã ã•ã„ã€ã¨ã„ã£ãŸèª¬æ˜ãŒä»˜ä¸ã•ã‚Œã¾ã™ã€‚<br>
ã“ã‚Œã«åŠ ãˆã€GPTã‚„Geminiã®ã‚ˆã†ãªæœ€æ–°ã®LLMã¯ã€å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è¨“ç·´ã•ã‚Œã¦ãŠã‚Šã€ãã®ä¸­ã«ã¯ã€Œå¤–éƒ¨ã®æƒ…å ±æºã‚’å‚ç…§ã—ã¦è³ªå•ã«ç­”ãˆã‚‹ã€ã¨ã„ã†ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚å«ã¾ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã¯ã€Œä»Šæ—¥ã®å¤©æ°—ã¯ï¼Ÿã€ã¨ã„ã£ãŸè³ªå•ãŒã€å˜ãªã‚‹çŸ¥è­˜ã§ã¯ãªãã€å¤–éƒ¨ãƒ„ãƒ¼ãƒ«ã‚’å¿…è¦ã¨ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã‚ã‚‹ã“ã¨ã‚’å­¦ç¿’ã—ã¦ã„ã¾ã™ã€‚åŒæ™‚ã«ã€LLMã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã€Œæ„å›³ã€ã‚’åˆ†æã—ã¾ã™ã€‚ä¾‹ãˆã°ã€ä»Šæ—¥ã®å¤©æ°—ã§ã‚ã‚Œã°ã€æ„å›³ã¨ã—ã¦æœ€æ–°ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æƒ…å ±ãŒæ±‚ã‚ã‚‰ã‚Œã¦ã„ã‚‹ã¨åˆ†æã—ã¾ã™ã€‚ã¾ãŸã€æ—¥æœ¬ã®é¦–éƒ½ã¯ã©ã“ï¼Ÿã¨ã„ã†è³ªå•ã§ã‚ã‚Œã°ã€ãã‚Œã¯æ—¢çŸ¥ã®çŸ¥è­˜ã§å›ç­”å¯èƒ½ã¨åˆ¤æ–­ã§ãã¾ã™ã€‚ã“ã®ãƒ—ãƒ­ã‚»ã‚¹ã¯ã€ã¾ã‚‹ã§äººé–“ãŒã€Œä»Šæ—¥ã®å¤©æ°—ã¯ï¼Ÿã€ã¨èã‹ã‚ŒãŸã¨ãã«ã€é ­ã®ä¸­ã®çŸ¥è­˜ã ã‘ã§ç­”ãˆãšã€ã‚¹ãƒãƒ›ã§å¤©æ°—äºˆå ±ã‚’èª¿ã¹ã‚‹ã‚ˆã†ã«ã€LLMãŒã‚¿ã‚¹ã‚¯ã®æ€§è³ªã‚’ç†è§£ã—ã€é©åˆ‡ãªãƒ„ãƒ¼ãƒ«ï¼ˆã‚¹ãƒãƒ›ï¼‰ã‚’é¸æŠã™ã‚‹ã®ã¨ä¼¼ã¦ã„ã¾ã™ã€‚<br>
LLMã®åˆ¤æ–­ã¯ç¢ºç‡çš„ãªã‚‚ã®ã§ã™ãŒã€ãã®ç¢ºç‡ã®èƒŒå¾Œã«ã¯ã€ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒæœ€ã‚‚åˆç†çš„ã§åŠ¹ç‡çš„ãªè§£æ±ºç­–ã§ã‚ã‚‹ã¨ã„ã†å¼·ã„æ ¹æ‹ ãŒå­˜åœ¨ã—ã¾ã™ã€‚ã—ãŸãŒã£ã¦ã€ã“ã‚Œã¯å˜ãªã‚‹ã€Œæ›–æ˜§ãªæ¨æ¸¬ã€ã§ã¯ãªãã€æ˜ç¢ºãªæ„å›³ã«åŸºã¥ãã€Œè«–ç†çš„ãªåˆ¤æ–­ã€ã¨ãªã£ã¦ã„ã¾ã™ã€‚


### 6-1. Chat Memory via MongoDB
You can use the MongoDB as a chat memory, so that the history will be saved in the MongoDB. See below:<br>
You should do configure like this.<br>
<img src="https://github.com/developer-onizuka/n8n-ollama/blob/main/n8n-mongodb.png" width="720">

Then, you can find the history of the chat you've made.<br>
```
$ kubectl exec -it mongodb-statefulset-0 -- mongosh --username admin --password password --authenticationDatabase admin
Current Mongosh Log ID:	68cd2031b32a8de04d4f87fd
Connecting to:		mongodb://<credentials>@127.0.0.1:27017/?directConnection=true&serverSelectionTimeoutMS=2000&authSource=admin&appName=mongosh+2.5.8
Using MongoDB:		8.0.13
Using Mongosh:		2.5.8

For mongosh info see: https://www.mongodb.com/docs/mongodb-shell/

------
   The server generated these startup warnings when booting
   2025-09-19T09:18:33.985+00:00: For customers running the current memory allocator, we suggest changing the contents of the following sysfsFile
   2025-09-19T09:18:33.985+00:00: For customers running the current memory allocator, we suggest changing the contents of the following sysfsFile
   2025-09-19T09:18:33.985+00:00: We suggest setting the contents of sysfsFile to 0.
   2025-09-19T09:18:33.985+00:00: vm.max_map_count is too low
------

test> show dbs
admin            100.00 KiB
config            12.00 KiB
local             72.00 KiB
n8n-chat-memory    8.00 KiB
```
```
test> use n8n-chat-memory
switched to db n8n-chat-memory
n8n-chat-memory> show collections
chat_sessions
n8n_chat_histories
n8n-chat-memory> db.n8n_chat_histories.find()
[
  {
    _id: ObjectId('68cd2e8410810d0090fabc4b'),
    sessionId: '7973f3ebff8c4f519862adf2bd4ba4c8',
    messages: [
      {
        type: 'human',
        data: {
          content: '## Steps to follow\n' +
            '\n' +
            '1. Skip \n' +
            '\n' +
            '\n' +
            '2. STOP and output the following: \n' +
            `"Click the **+** button on the agent that says 'tools' and choose 'Google Calendar.'" \n` +
            ' ----- IGNORE BELOW -----\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '8. If all steps are completed, output the following:\n' +
            '"Would you like me to check all events in your calendar for tomorrow 2025-09-20?"\n' +
            '\n' +
            '# User message\n' +
            '\n' +
            'Hello. My name is developer-onizuka.',
          additional_kwargs: {},
          response_metadata: {}
        }
      },
      {
        type: 'ai',
        data: {
          content: "Nice to meet you, Developer-Onizuka! ğŸ‘‹ Let's get started with setting up our schedule. Please click the **+** button on the agent that says 'tools' and choose 'Google Calendar'. âœ¨",
          tool_calls: [],
          invalid_tool_calls: [],
          additional_kwargs: {},
          response_metadata: {}
        }
      },
      {
        type: 'human',
        data: {
          content: '## Steps to follow\n' +
            '\n' +
            '1. Skip \n' +
            '\n' +
            '\n' +
            '2. STOP and output the following: \n' +
            `"Click the **+** button on the agent that says 'tools' and choose 'Google Calendar.'" \n` +
            ' ----- IGNORE BELOW -----\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '8. If all steps are completed, output the following:\n' +
            '"Would you like me to check all events in your calendar for tomorrow 2025-09-20?"\n' +
            '\n' +
            '# User message\n' +
            '\n' +
            'Do you remember my name?',
          additional_kwargs: {},
          response_metadata: {}
        }
      },
      {
        type: 'ai',
        data: {
          content: "I'm sorry that slips my mind! Let's get back on track. Click the **+** button on the agent that says 'tools' and choose 'Google Calendar'. âœ¨",
          tool_calls: [],
          invalid_tool_calls: [],
          additional_kwargs: {},
          response_metadata: {}
        }
      },
      {
        type: 'human',
        data: {
          content: '## Steps to follow\n' +
            '\n' +
            '1. Skip \n' +
            '\n' +
            '\n' +
            '2. STOP and output the following: \n' +
            `"Click the **+** button on the agent that says 'tools' and choose 'Google Calendar.'" \n` +
            ' ----- IGNORE BELOW -----\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '\n' +
            '8. If all steps are completed, output the following:\n' +
            '"Would you like me to check all events in your calendar for tomorrow 2025-09-20?"\n' +
            '\n' +
            '# User message\n' +
            '\n' +
            'Do you remember my name?',
          additional_kwargs: {},
          response_metadata: {}
        }
      },
      {
        type: 'ai',
        data: {
          content: "My apologies again! Yes, I recall that it's developer-onizuka. Please click the **+** button on the agent that says 'tools' and choose 'Google Calendar'. âœ¨",
          tool_calls: [],
          invalid_tool_calls: [],
          additional_kwargs: {},
          response_metadata: {}
        }
      }
    ]
  }
]
```

### 6-2. Calculator for LLM
As you know LLM is not good at mathmatics, so you should deploy the calculator for the case of calculation.<br>

<img src="https://github.com/developer-onizuka/n8n-ollama/blob/main/n8n-square-root.png" width="720">


